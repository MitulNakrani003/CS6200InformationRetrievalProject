{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Set, Any\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank_bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from rank_bm25) (2.2.2)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank_bm25\n",
      "Successfully installed rank_bm25-0.2.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Working directory set to: /Users/shailshah/Desktop/NEU MSCS/Sem4/IR/Final Project\n",
      "✅ Current working directory: /Users/shailshah/Desktop/NEU MSCS/Sem4/IR/Final Project/CS6200InformationRetrievalProject\n"
     ]
    }
   ],
   "source": [
    "print(\"✅ Working directory set to:\", os.getcwd())\n",
    "os.chdir(os.path.join(os.getcwd(), \"CS6200InformationRetrievalProject\"))\n",
    "print(\"✅ Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> List[str]:\n",
    "    return text.lower().split()\n",
    "\n",
    "def load_documents(file_path: str) -> Tuple[List[Dict[str, Any]], List[List[str]]]:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    raw_docs = []\n",
    "    tokenized_docs = []\n",
    "    \n",
    "    # Extract documents from the JSON structure\n",
    "    if 'documents' in data:\n",
    "        for doc in data['documents']:\n",
    "            if 'document_content' in doc:\n",
    "                # Check if document_content is a JSON string\n",
    "                if isinstance(doc['document_content'], str):\n",
    "                    try:\n",
    "                        content_json = json.loads(doc['document_content'])\n",
    "                        if 'documents' in content_json:\n",
    "                            for inner_doc in content_json['documents']:\n",
    "                                doc_content = inner_doc.get('content', '')\n",
    "                                raw_docs.append({\n",
    "                                    'doc_id': inner_doc.get('doc_id', len(raw_docs) + 1),\n",
    "                                    'title': inner_doc.get('title', ''),\n",
    "                                    'content': doc_content\n",
    "                                })\n",
    "                                tokenized_docs.append(preprocess_text(doc_content))\n",
    "                    except json.JSONDecodeError:\n",
    "                        # Treat as regular text\n",
    "                        doc_content = doc['document_content']\n",
    "                        raw_docs.append({\n",
    "                            'doc_id': doc.get('index', len(raw_docs) + 1),\n",
    "                            'title': doc.get('source', ''),\n",
    "                            'content': doc_content\n",
    "                        })\n",
    "                        tokenized_docs.append(preprocess_text(doc_content))\n",
    "                else:\n",
    "                    # Handle JSON content\n",
    "                    if 'documents' in doc['document_content']:\n",
    "                        for inner_doc in doc['document_content']['documents']:\n",
    "                            doc_content = inner_doc.get('content', '')\n",
    "                            raw_docs.append({\n",
    "                                'doc_id': inner_doc.get('doc_id', len(raw_docs) + 1),\n",
    "                                'title': inner_doc.get('title', ''),\n",
    "                                'content': doc_content\n",
    "                            })\n",
    "                            tokenized_docs.append(preprocess_text(doc_content))\n",
    "    \n",
    "    # If no documents were found with the above structure, try a different approach\n",
    "    if len(raw_docs) == 0:\n",
    "        if 'documents' in data:\n",
    "            for doc in data['documents']:\n",
    "                if isinstance(doc, dict):\n",
    "                    doc_content = doc.get('content', '')\n",
    "                    raw_docs.append({\n",
    "                        'doc_id': doc.get('doc_id', len(raw_docs) + 1),\n",
    "                        'title': doc.get('title', ''),\n",
    "                        'content': doc_content\n",
    "                    })\n",
    "                    tokenized_docs.append(preprocess_text(doc_content))\n",
    "    \n",
    "    return raw_docs, tokenized_docs\n",
    "\n",
    "def load_queries(file_path: str) -> List[Dict[str, Any]]:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    if 'queries' in data:\n",
    "        return data['queries']\n",
    "    return []\n",
    "\n",
    "def rank_queries(bm25: BM25Okapi, queries: List[Dict[str, Any]], raw_docs: List[Dict[str, Any]]) -> Dict[int, List[Tuple[int, float]]]:\n",
    "    rankings = {}\n",
    "    \n",
    "    for query in queries:\n",
    "        query_id = query['query_id']\n",
    "        query_text = query['query']\n",
    "        tokenized_query = preprocess_text(query_text)\n",
    "        \n",
    "        scores = bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        # Create (doc_id, score) pairs\n",
    "        ranked_docs = [(raw_docs[idx]['doc_id'], score) for idx, score in enumerate(scores)]\n",
    "        \n",
    "        # Sort by score in descending order\n",
    "        ranked_docs = sorted(ranked_docs, key=lambda x: x[1], reverse=True)\n",
    "        rankings[query_id] = ranked_docs\n",
    "    return rankings\n",
    "\n",
    "def plot_rankings(rankings: Dict[int, List[Tuple[int, float]]], output_dir: str, file_name: str):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Plot 1: Distribution of scores for top 10 documents for each query\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    all_query_ids = sorted(rankings.keys())\n",
    "    for query_id in all_query_ids:\n",
    "        top_docs = rankings[query_id][:10]  # Top 10 documents\n",
    "        scores = [score for _, score in top_docs]\n",
    "        \n",
    "        plt.plot(range(1, len(scores) + 1), scores, marker='o', label=f'Query {query_id}')\n",
    "    \n",
    "    plt.xlabel('Document Rank')\n",
    "    plt.ylabel('BM25 Score')\n",
    "    plt.title(f'Top 10 Document Scores per Query for {file_name}')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/{file_name}_score_distribution.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 2: Heatmap of document relevance across queries\n",
    "    query_count = len(all_query_ids)\n",
    "    doc_count = 20  # Only show top 20 documents per query for clarity\n",
    "    \n",
    "    # Create a matrix for the heatmap\n",
    "    heatmap_data = np.zeros((query_count, doc_count))\n",
    "    \n",
    "    for i, query_id in enumerate(all_query_ids):\n",
    "        top_docs = rankings[query_id][:doc_count]\n",
    "        for j, (doc_id, score) in enumerate(top_docs):\n",
    "            heatmap_data[i, j] = score\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.imshow(heatmap_data, cmap='viridis', aspect='auto')\n",
    "    plt.colorbar(label='BM25 Score')\n",
    "    plt.xlabel('Document Rank')\n",
    "    plt.ylabel('Query ID')\n",
    "    plt.title(f'BM25 Score Heatmap for {file_name}')\n",
    "    plt.yticks(range(query_count), all_query_ids)\n",
    "    plt.xticks(range(doc_count), range(1, doc_count + 1))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/{file_name}_heatmap.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 3: Average score by rank position\n",
    "    avg_scores = []\n",
    "    for rank in range(doc_count):\n",
    "        scores_at_rank = []\n",
    "        for query_id in all_query_ids:\n",
    "            if rank < len(rankings[query_id]):\n",
    "                scores_at_rank.append(rankings[query_id][rank][1])\n",
    "        \n",
    "        if scores_at_rank:\n",
    "            avg_scores.append(np.mean(scores_at_rank))\n",
    "        else:\n",
    "            avg_scores.append(0)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(avg_scores) + 1), avg_scores, marker='o', linestyle='-', color='blue')\n",
    "    plt.xlabel('Document Rank')\n",
    "    plt.ylabel('Average BM25 Score')\n",
    "    plt.title(f'Average BM25 Score by Rank Position for {file_name}')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/{file_name}_avg_score_by_rank.png\")\n",
    "    plt.close()\n",
    "\n",
    "def write_rankings_to_file(rankings: Dict[int, List[Tuple[int, float]]], output_dir: str, file_name: str, raw_docs: List[Dict[str, Any]], queries: List[Dict[str, Any]]):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create a mapping of doc_id to document info for quick lookup\n",
    "    doc_id_to_info = {doc['doc_id']: doc for doc in raw_docs}\n",
    "    \n",
    "    # Create a mapping of query_id to query text for quick lookup\n",
    "    query_id_to_text = {query['query_id']: query['query'] for query in queries}\n",
    "    \n",
    "    with open(f\"{output_dir}Results/{file_name}_rankings.txt\", 'w') as f:\n",
    "        for query_id, docs in sorted(rankings.items()):\n",
    "            query_text = query_id_to_text.get(query_id, \"Unknown query\")\n",
    "            f.write(f\"Query {query_id}: {query_text}\\n\")\n",
    "            for rank, (doc_id, score) in enumerate(docs, 1):\n",
    "                title = doc_id_to_info.get(doc_id, {}).get('title', 'Unknown Title')\n",
    "                f.write(f\"  Rank {rank}: Document {doc_id} - Title: {title} - Score: {score:.4f}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "def compare_rankings(all_rankings: Dict[str, Dict[int, List[Tuple[int, float]]]], output_dir: str):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all query IDs\n",
    "    all_query_ids = set()\n",
    "    for rankings in all_rankings.values():\n",
    "        all_query_ids.update(rankings.keys())\n",
    "    all_query_ids = sorted(all_query_ids)\n",
    "    \n",
    "    # Compare average scores for the top 10 documents across files\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    bar_width = 0.8 / len(all_rankings)\n",
    "    index = np.arange(len(all_query_ids))\n",
    "    \n",
    "    for i, (file_name, rankings) in enumerate(sorted(all_rankings.items())):\n",
    "        avg_scores = []\n",
    "        \n",
    "        for query_id in all_query_ids:\n",
    "            if query_id in rankings:\n",
    "                top_docs = rankings[query_id][:10]  # Top 10 documents\n",
    "                if top_docs:\n",
    "                    avg_scores.append(np.mean([score for _, score in top_docs]))\n",
    "                else:\n",
    "                    avg_scores.append(0)\n",
    "            else:\n",
    "                avg_scores.append(0)\n",
    "        \n",
    "        plt.bar(index + i * bar_width, avg_scores, bar_width, label=file_name)\n",
    "    \n",
    "    plt.xlabel('Query ID')\n",
    "    plt.ylabel('Average BM25 Score (Top 10 Documents)')\n",
    "    plt.title('Comparison of Average BM25 Scores Across Document Collections')\n",
    "    plt.xticks(index + bar_width * (len(all_rankings) - 1) / 2, all_query_ids)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/comparison_avg_scores.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Compare number of documents with scores above threshold\n",
    "    threshold = 5.0  # Example threshold\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    for i, (file_name, rankings) in enumerate(sorted(all_rankings.items())):\n",
    "        high_scoring_docs = []\n",
    "        \n",
    "        for query_id in all_query_ids:\n",
    "            if query_id in rankings:\n",
    "                count = sum(1 for _, score in rankings[query_id] if score > threshold)\n",
    "                high_scoring_docs.append(count)\n",
    "            else:\n",
    "                high_scoring_docs.append(0)\n",
    "        \n",
    "        plt.bar(index + i * bar_width, high_scoring_docs, bar_width, label=file_name)\n",
    "    \n",
    "    plt.xlabel('Query ID')\n",
    "    plt.ylabel(f'Number of Documents with Score > {threshold}')\n",
    "    plt.title(f'Comparison of High-Scoring Documents Across Collections (Threshold: {threshold})')\n",
    "    plt.xticks(index + bar_width * (len(all_rankings) - 1) / 2, all_query_ids)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/comparison_high_scoring_docs.png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_relevance_judgments(queries: List[Dict[str, Any]], raw_docs: List[Dict[str, Any]]) -> Dict[int, Dict[int, float]]:\n",
    "    from collections import Counter\n",
    "    import math\n",
    "    \n",
    "    def compute_tf(text_tokens):\n",
    "        \"\"\"Compute term frequency\"\"\"\n",
    "        tf_dict = Counter(text_tokens)\n",
    "        # Normalize by document length\n",
    "        for term in tf_dict:\n",
    "            tf_dict[term] = tf_dict[term] / len(text_tokens)\n",
    "        return tf_dict\n",
    "    \n",
    "    def compute_idf(corpus_tokens):\n",
    "        \"\"\"Compute inverse document frequency\"\"\"\n",
    "        idf_dict = {}\n",
    "        num_docs = len(corpus_tokens)\n",
    "        \n",
    "        # Count documents containing each term\n",
    "        doc_count = Counter()\n",
    "        for doc_tokens in corpus_tokens:\n",
    "            unique_terms = set(doc_tokens)\n",
    "            for term in unique_terms:\n",
    "                doc_count[term] += 1\n",
    "        \n",
    "        # Calculate IDF for each term\n",
    "        for term, count in doc_count.items():\n",
    "            idf_dict[term] = math.log(num_docs / (1 + count))\n",
    "        \n",
    "        return idf_dict\n",
    "    \n",
    "    def compute_tfidf(tf_dict, idf_dict):\n",
    "        \"\"\"Compute TF-IDF scores\"\"\"\n",
    "        tfidf_dict = {}\n",
    "        for term, tf_value in tf_dict.items():\n",
    "            tfidf_dict[term] = tf_value * idf_dict.get(term, 0)\n",
    "        return tfidf_dict\n",
    "    \n",
    "    def cosine_similarity(vec1, vec2):\n",
    "        \"\"\"Compute cosine similarity between two vectors\"\"\"\n",
    "        # Find common terms\n",
    "        common_terms = set(vec1.keys()) & set(vec2.keys())\n",
    "        \n",
    "        # Calculate dot product\n",
    "        dot_product = sum(vec1[term] * vec2[term] for term in common_terms)\n",
    "        \n",
    "        # Calculate magnitudes\n",
    "        magnitude1 = math.sqrt(sum(value ** 2 for value in vec1.values()))\n",
    "        magnitude2 = math.sqrt(sum(value ** 2 for value in vec2.values()))\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if magnitude1 == 0 or magnitude2 == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Return cosine similarity\n",
    "        return dot_product / (magnitude1 * magnitude2)\n",
    "    \n",
    "    relevance_judgments = {}\n",
    "    \n",
    "    # Preprocess all documents for IDF calculation\n",
    "    all_doc_tokens = [preprocess_text(doc.get('content', '').lower()) for doc in raw_docs]\n",
    "    idf_dict = compute_idf(all_doc_tokens)\n",
    "    \n",
    "    # Process each query\n",
    "    for query in queries:\n",
    "        query_id = query['query_id']\n",
    "        query_text = query['query'].lower()\n",
    "        narrative = query.get('narrative', '').lower()\n",
    "        \n",
    "        # Initialize relevance judgments for this query\n",
    "        relevance_judgments[query_id] = {}\n",
    "        \n",
    "        # Combine query and narrative for a more comprehensive representation\n",
    "        # With higher weight for query terms\n",
    "        combined_query = query_text + \" \" + query_text + \" \" + narrative\n",
    "        query_tokens = preprocess_text(combined_query)\n",
    "        \n",
    "        # Compute query TF-IDF\n",
    "        query_tf = compute_tf(query_tokens)\n",
    "        query_tfidf = compute_tfidf(query_tf, idf_dict)\n",
    "        \n",
    "        # For each document, compute relevance\n",
    "        for i, doc in enumerate(raw_docs):\n",
    "            doc_id = doc['doc_id']\n",
    "            title = doc.get('title', '').lower()\n",
    "            content = doc.get('content', '').lower()\n",
    "            \n",
    "            # Give title terms more weight\n",
    "            combined_doc = title + \" \" + title + \" \" + content\n",
    "            doc_tokens = preprocess_text(combined_doc)\n",
    "            \n",
    "            # Compute document TF-IDF\n",
    "            doc_tf = compute_tf(doc_tokens)\n",
    "            doc_tfidf = compute_tfidf(doc_tf, idf_dict)\n",
    "            \n",
    "            # Calculate relevance using cosine similarity\n",
    "            similarity = cosine_similarity(query_tfidf, doc_tfidf)\n",
    "            \n",
    "            # Additional boost for exact title match\n",
    "            if query_text in title:\n",
    "                similarity += 0.2\n",
    "                \n",
    "            # Clamp to [0, 1] range\n",
    "            relevance = min(1.0, max(0.0, similarity))\n",
    "            \n",
    "            # Threshold - only include if somewhat relevant (stricter threshold)\n",
    "            if relevance > 0.15:\n",
    "                relevance_judgments[query_id][doc_id] = relevance\n",
    "    \n",
    "    return relevance_judgments\n",
    "\n",
    "def mean_average_precision_at_k(rankings: Dict[int, List[Tuple[int, float]]], relevance_judgments: Dict[int, Dict[int, float]], k: int = 10) -> Dict[int, float]:\n",
    "    map_at_k = {}\n",
    "    \n",
    "    for query_id, ranked_docs in rankings.items():\n",
    "        if query_id not in relevance_judgments:\n",
    "            map_at_k[query_id] = 0.0\n",
    "            continue\n",
    "        \n",
    "        relevant_docs = relevance_judgments[query_id]\n",
    "        \n",
    "        precisions = []\n",
    "        relevant_count = 0\n",
    "        \n",
    "        for i, (doc_id, _) in enumerate(ranked_docs[:k]):\n",
    "            if doc_id in relevant_docs:\n",
    "                relevant_count += 1\n",
    "                precisions.append(relevant_count / (i + 1))\n",
    "        \n",
    "        if len(precisions) > 0:\n",
    "            map_at_k[query_id] = sum(precisions) / len(precisions)\n",
    "        else:\n",
    "            map_at_k[query_id] = 0.0\n",
    "    \n",
    "    return map_at_k\n",
    "\n",
    "def recall_at_k(rankings: Dict[int, List[Tuple[int, float]]], relevance_judgments: Dict[int, Dict[int, float]], k: int = 10) -> Dict[int, float]:\n",
    "    recall_at_k = {}\n",
    "    \n",
    "    for query_id, ranked_docs in rankings.items():\n",
    "        if query_id not in relevance_judgments:\n",
    "            recall_at_k[query_id] = 0.0\n",
    "            continue\n",
    "        \n",
    "        relevant_docs = relevance_judgments[query_id]\n",
    "        \n",
    "        if len(relevant_docs) == 0:\n",
    "            recall_at_k[query_id] = 0.0\n",
    "            continue\n",
    "        \n",
    "        retrieved_relevant = sum(1 for doc_id, _ in ranked_docs[:k] if doc_id in relevant_docs)\n",
    "        recall_at_k[query_id] = retrieved_relevant / len(relevant_docs)\n",
    "    \n",
    "    return recall_at_k\n",
    "\n",
    "def dcg_at_k(rankings: Dict[int, List[Tuple[int, float]]], relevance_judgments: Dict[int, Dict[int, float]], k: int = 10) -> Dict[int, float]:\n",
    "    dcg_at_k = {}\n",
    "    \n",
    "    for query_id, ranked_docs in rankings.items():\n",
    "        if query_id not in relevance_judgments:\n",
    "            dcg_at_k[query_id] = 0.0\n",
    "            continue\n",
    "        \n",
    "        relevant_docs = relevance_judgments[query_id]\n",
    "        \n",
    "        dcg = 0.0\n",
    "        for i, (doc_id, _) in enumerate(ranked_docs[:k]):\n",
    "            # Use binary relevance for simplicity (1 if relevant, 0 if not)\n",
    "            rel = relevant_docs.get(doc_id, 0.0)\n",
    "            \n",
    "            # Position is 1-based\n",
    "            position = i + 1\n",
    "            \n",
    "            # DCG formula\n",
    "            dcg += (2 ** rel - 1) / math.log2(position + 1)\n",
    "        \n",
    "        dcg_at_k[query_id] = dcg\n",
    "    \n",
    "    return dcg_at_k\n",
    "\n",
    "def idcg_at_k(relevance_judgments: Dict[int, Dict[int, float]], k: int = 10) -> Dict[int, float]:\n",
    "    idcg_at_k = {}\n",
    "    \n",
    "    for query_id, relevant_docs in relevance_judgments.items():\n",
    "        # Sort relevance scores in descending order\n",
    "        sorted_relevance = sorted(relevant_docs.values(), reverse=True)\n",
    "        \n",
    "        idcg = 0.0\n",
    "        for i, rel in enumerate(sorted_relevance[:k]):\n",
    "            # Position is 1-based\n",
    "            position = i + 1\n",
    "            \n",
    "            # IDCG formula\n",
    "            idcg += (2 ** rel - 1) / math.log2(position + 1)\n",
    "        \n",
    "        idcg_at_k[query_id] = idcg\n",
    "    \n",
    "    return idcg_at_k\n",
    "\n",
    "def ndcg_at_k(rankings: Dict[int, List[Tuple[int, float]]], \n",
    "              relevance_judgments: Dict[int, Dict[int, float]], \n",
    "              k: int = 10) -> Dict[int, float]:\n",
    "    dcg = dcg_at_k(rankings, relevance_judgments, k)\n",
    "    idcg = idcg_at_k(relevance_judgments, k)\n",
    "    \n",
    "    ndcg_at_k = {}\n",
    "    \n",
    "    for query_id in rankings.keys():\n",
    "        if query_id not in idcg or idcg[query_id] == 0.0:\n",
    "            ndcg_at_k[query_id] = 0.0\n",
    "        else:\n",
    "            ndcg_at_k[query_id] = dcg[query_id] / idcg[query_id]\n",
    "    \n",
    "    return ndcg_at_k\n",
    "\n",
    "def calculate_metrics(rankings: Dict[int, List[Tuple[int, float]]], relevance_judgments: Dict[int, Dict[int, float]], k: int = 10) -> Dict[str, Dict[int, float]]:\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['map'] = mean_average_precision_at_k(rankings, relevance_judgments, k)\n",
    "    metrics['recall'] = recall_at_k(rankings, relevance_judgments, k)\n",
    "    metrics['ndcg'] = ndcg_at_k(rankings, relevance_judgments, k)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_metrics(metrics: Dict[str, Dict[int, float]], output_dir: str, file_name: str):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    all_query_ids = sorted(set().union(*[set(m.keys()) for m in metrics.values()]))\n",
    "    \n",
    "    # Plot 1: Bar chart of MAP@10 for each query\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    map_values = [metrics['map'].get(query_id, 0.0) for query_id in all_query_ids]\n",
    "    plt.bar(range(len(all_query_ids)), map_values, color='blue', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Query ID')\n",
    "    plt.ylabel('MAP@10')\n",
    "    plt.title(f'Mean Average Precision@10 per Query for {file_name}')\n",
    "    plt.xticks(range(len(all_query_ids)), all_query_ids)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/{file_name}_map_at_10.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 2: Bar chart of Recall@10 for each query\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    recall_values = [metrics['recall'].get(query_id, 0.0) for query_id in all_query_ids]\n",
    "    plt.bar(range(len(all_query_ids)), recall_values, color='green', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Query ID')\n",
    "    plt.ylabel('Recall@10')\n",
    "    plt.title(f'Recall@10 per Query for {file_name}')\n",
    "    plt.xticks(range(len(all_query_ids)), all_query_ids)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/{file_name}_recall_at_10.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 3: Bar chart of NDCG@10 for each query\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    ndcg_values = [metrics['ndcg'].get(query_id, 0.0) for query_id in all_query_ids]\n",
    "    plt.bar(range(len(all_query_ids)), ndcg_values, color='purple', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Query ID')\n",
    "    plt.ylabel('NDCG@10')\n",
    "    plt.title(f'Normalized Discounted Cumulative Gain@10 per Query for {file_name}')\n",
    "    plt.xticks(range(len(all_query_ids)), all_query_ids)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/{file_name}_ndcg_at_10.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 4: Combined metric comparison\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    x = np.arange(len(all_query_ids))\n",
    "    width = 0.25\n",
    "    \n",
    "    plt.bar(x - width, map_values, width, label='MAP@10', color='blue', alpha=0.7)\n",
    "    plt.bar(x, recall_values, width, label='Recall@10', color='green', alpha=0.7)\n",
    "    plt.bar(x + width, ndcg_values, width, label='NDCG@10', color='purple', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Query ID')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.title(f'Evaluation Metrics Comparison for {file_name}')\n",
    "    plt.xticks(x, all_query_ids)\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/{file_name}_metrics_comparison.png\")\n",
    "    plt.close()\n",
    "\n",
    "def write_metrics_to_file(metrics: Dict[str, Dict[int, float]], output_dir: str, file_name: str):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    with open(f\"{output_dir}Results/{file_name}_metrics.txt\", 'w') as f:\n",
    "        all_query_ids = sorted(set().union(*[set(m.keys()) for m in metrics.values()]))\n",
    "        \n",
    "        f.write(f\"Evaluation Metrics for {file_name}:\\n\\n\")\n",
    "        \n",
    "        # Per-query metrics\n",
    "        f.write(\"Per-Query Metrics:\\n\")\n",
    "        f.write(\"Query ID | MAP@10  | Recall@10 | NDCG@10\\n\")\n",
    "        f.write(\"-\" * 45 + \"\\n\")\n",
    "        \n",
    "        for query_id in all_query_ids:\n",
    "            map_value = metrics['map'].get(query_id, 0.0)\n",
    "            recall_value = metrics['recall'].get(query_id, 0.0)\n",
    "            ndcg_value = metrics['ndcg'].get(query_id, 0.0)\n",
    "            \n",
    "            f.write(f\"{query_id:8} | {map_value:.4f} | {recall_value:.4f}  | {ndcg_value:.4f}\\n\")\n",
    "        \n",
    "        # Overall averages\n",
    "        f.write(\"\\nOverall Averages:\\n\")\n",
    "        avg_map = sum(metrics['map'].values()) / len(metrics['map']) if metrics['map'] else 0.0\n",
    "        avg_recall = sum(metrics['recall'].values()) / len(metrics['recall']) if metrics['recall'] else 0.0\n",
    "        avg_ndcg = sum(metrics['ndcg'].values()) / len(metrics['ndcg']) if metrics['ndcg'] else 0.0\n",
    "        \n",
    "        f.write(f\"Average MAP@10:     {avg_map:.4f}\\n\")\n",
    "        f.write(f\"Average Recall@10:  {avg_recall:.4f}\\n\")\n",
    "        f.write(f\"Average NDCG@10:    {avg_ndcg:.4f}\\n\")\n",
    "\n",
    "def compare_metrics(all_metrics: Dict[str, Dict[str, Dict[int, float]]], output_dir: str):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all metric types and query IDs\n",
    "    metric_types = set()\n",
    "    all_query_ids = set()\n",
    "    \n",
    "    for file_metrics in all_metrics.values():\n",
    "        metric_types.update(file_metrics.keys())\n",
    "        for metric_values in file_metrics.values():\n",
    "            all_query_ids.update(metric_values.keys())\n",
    "    \n",
    "    metric_types = sorted(metric_types)\n",
    "    all_query_ids = sorted(all_query_ids)\n",
    "    \n",
    "    # Compare average metrics across files\n",
    "    for metric_type in metric_types:\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        bar_width = 0.8 / len(all_metrics)\n",
    "        index = np.arange(len(all_query_ids))\n",
    "        \n",
    "        for i, (file_name, file_metrics) in enumerate(sorted(all_metrics.items())):\n",
    "            metric_values = []\n",
    "            \n",
    "            for query_id in all_query_ids:\n",
    "                if metric_type in file_metrics and query_id in file_metrics[metric_type]:\n",
    "                    metric_values.append(file_metrics[metric_type][query_id])\n",
    "                else:\n",
    "                    metric_values.append(0.0)\n",
    "            \n",
    "            plt.bar(index + i * bar_width, metric_values, bar_width, label=file_name)\n",
    "        \n",
    "        plt.xlabel('Query ID')\n",
    "        plt.ylabel(f'{metric_type.upper()}@10')\n",
    "        plt.title(f'Comparison of {metric_type.upper()}@10 Across Document Collections')\n",
    "        plt.xticks(index + bar_width * (len(all_metrics) - 1) / 2, all_query_ids)\n",
    "        plt.legend(loc='best')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/comparison_{metric_type}_at_10.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    # Compare overall average metrics across files\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    bar_width = 0.8 / len(metric_types)\n",
    "    index = np.arange(len(all_metrics))\n",
    "    \n",
    "    for i, metric_type in enumerate(metric_types):\n",
    "        avg_metrics = []\n",
    "        \n",
    "        for file_name, file_metrics in sorted(all_metrics.items()):\n",
    "            if metric_type in file_metrics:\n",
    "                avg_metric = sum(file_metrics[metric_type].values()) / len(file_metrics[metric_type]) if file_metrics[metric_type] else 0.0\n",
    "                avg_metrics.append(avg_metric)\n",
    "            else:\n",
    "                avg_metrics.append(0.0)\n",
    "        \n",
    "        plt.bar(index + i * bar_width, avg_metrics, bar_width, label=f'{metric_type.upper()}@10')\n",
    "    \n",
    "    plt.xlabel('Document Collection')\n",
    "    plt.ylabel('Average Metric Value')\n",
    "    plt.title('Comparison of Average Evaluation Metrics Across Document Collections')\n",
    "    plt.xticks(index + bar_width * (len(metric_types) - 1) / 2, [file_name for file_name in sorted(all_metrics.keys())])\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/comparison_avg_metrics.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing documentsOriginal...\n",
      "Processing documentsBart...\n",
      "Processing documentsLongformer...\n",
      "Processing documentsT5...\n",
      "All processing complete!\n"
     ]
    }
   ],
   "source": [
    "# Directory paths\n",
    "input_dir = \"data\"\n",
    "output_dir = \"rankedoutputBM25\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load queries\n",
    "queries_file = f\"{input_dir}/queries.json\"\n",
    "queries = load_queries(queries_file)\n",
    "\n",
    "# Process all document files\n",
    "document_files = [f for f in os.listdir(input_dir) if f.startswith(\"documents\") and f.endswith(\".json\")]\n",
    "\n",
    "all_rankings = {}\n",
    "all_metrics = {}\n",
    "\n",
    "for doc_file in document_files:\n",
    "    file_path = f\"{input_dir}/{doc_file}\"\n",
    "    file_name = os.path.splitext(doc_file)[0]\n",
    "    \n",
    "    print(f\"Processing {file_name}...\")\n",
    "    \n",
    "    # Load documents\n",
    "    raw_docs, tokenized_docs = load_documents(file_path)\n",
    "    \n",
    "    if len(raw_docs) == 0:\n",
    "        print(f\"No documents found in {file_name}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Initialize BM25Okapi\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    \n",
    "    # Rank queries\n",
    "    rankings = rank_queries(bm25, queries, raw_docs)\n",
    "    \n",
    "    # Create pseudo-relevance judgments\n",
    "    relevance_judgments = create_relevance_judgments(queries, raw_docs)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    metrics = calculate_metrics(rankings, relevance_judgments)\n",
    "    \n",
    "    # Store rankings and metrics for comparison\n",
    "    all_rankings[file_name] = rankings\n",
    "    all_metrics[file_name] = metrics\n",
    "    \n",
    "    # Generate plots for this document collection\n",
    "    plot_rankings(rankings, output_dir, file_name)\n",
    "    plot_metrics(metrics, output_dir, file_name)\n",
    "    \n",
    "    # Write rankings and metrics to file\n",
    "    write_rankings_to_file(rankings, output_dir, file_name, raw_docs, queries)\n",
    "    write_metrics_to_file(metrics, output_dir, file_name)\n",
    "\n",
    "# Compare rankings and metrics across document collections\n",
    "if len(all_rankings) > 1:\n",
    "    compare_rankings(all_rankings, output_dir)\n",
    "    compare_metrics(all_metrics, output_dir)\n",
    "\n",
    "print(\"All processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
